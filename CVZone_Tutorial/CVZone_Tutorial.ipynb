{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **CVZone Tutorial**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CVZone:**\n",
    "This is a Computer vision package that makes its easy to run Image processing and AI functions. At the core it uses OpenCV and Mediapipe libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Reference: https://github.com/cvzone/cvzone**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Installations**\n",
    "\n",
    "To install the cvzone package, run the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install cvzone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Requirements**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install numpy opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import cvzone\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Corner Rectangle**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function cornerRect in module cvzone.Utils:\n",
      "\n",
      "cornerRect(img, bbox, l=30, t=5, rt=1, colorR=(255, 0, 255), colorC=(0, 255, 0))\n",
      "    :param img: Image to draw on.\n",
      "    :param bbox: Bounding box [x, y, w, h]\n",
      "    :param l: length of the corner line\n",
      "    :param t: thickness of the corner line\n",
      "    :param rt: thickness of the rectangle\n",
      "    :param colorR: Color of the Rectangle\n",
      "    :param colorC: Color of the Corners\n",
      "    :return:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(cvzone.cornerRect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import cvzone\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Add a rectangle with styled corners to the image\n",
    "    frame = cvzone.cornerRect(\n",
    "        frame,\n",
    "        (200, 200, 300, 200),\n",
    "        l=30,\n",
    "        t=5,\n",
    "        rt=1,\n",
    "        colorR=(255, 0, 255),\n",
    "        colorC=(0, 255, 0),\n",
    "    )\n",
    "\n",
    "    cv2.imshow(\"Image\", frame)\n",
    "\n",
    "    if cv2.waitKey(1) == ord(\"q\"):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **PutTextRect**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function putTextRect in module cvzone.Utils:\n",
      "\n",
      "putTextRect(img, text, pos, scale=3, thickness=3, colorT=(255, 255, 255), colorR=(255, 0, 255), font=1, offset=10, border=None, colorB=(0, 255, 0))\n",
      "    Creates Text with Rectangle Background\n",
      "    :param img: Image to put text rect on\n",
      "    :param text: Text inside the rect\n",
      "    :param pos: Starting position of the rect x1,y1\n",
      "    :param scale: Scale of the text\n",
      "    :param thickness: Thickness of the text\n",
      "    :param colorT: Color of the Text\n",
      "    :param colorR: Color of the Rectangle\n",
      "    :param font: Font used. Must be cv2.FONT....\n",
      "    :param offset: Clearance around the text\n",
      "    :param border: Outline around the rect\n",
      "    :param colorB: Color of the outline\n",
      "    :return: image, rect (x1,y1,x2,y2)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(cvzone.putTextRect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import cvzone\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Add a rectangle and put text inside it on the image\n",
    "    cvzone.putTextRect(\n",
    "        frame,\n",
    "        \"Raafat\",\n",
    "        (50, 100),\n",
    "        scale=2,\n",
    "        thickness=2,\n",
    "        colorT=(255, 255, 255),\n",
    "        colorR=(255, 0, 255),\n",
    "        font=2,\n",
    "        offset=10,\n",
    "        border=2,\n",
    "        colorB=(0, 0, 0),\n",
    "    )\n",
    "\n",
    "    cv2.imshow(\"Image\", frame)\n",
    "\n",
    "    if cv2.waitKey(1) == ord(\"q\"):\n",
    "        break\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Download Image from URL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function downloadImageFromUrl in module cvzone.Utils:\n",
      "\n",
      "downloadImageFromUrl(url, keepTransparency=False)\n",
      "    Download an image from a given URL and return it as an OpenCV image.\n",
      "\n",
      "    :param url: The URL of the image to download\n",
      "    :param keep_transparency: Whether to keep the alpha channel (transparency) in the image (default: False)\n",
      "    :return: The downloaded image in OpenCV format\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(cvzone.downloadImageFromUrl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import cvzone\n",
    "\n",
    "imgNormal = cvzone.downloadImageFromUrl(\n",
    "    url=\"https://github.com/cvzone/cvzone/blob/master/Results/shapes.png?raw=true\"\n",
    ")\n",
    "\n",
    "imgPNG = cvzone.downloadImageFromUrl(\n",
    "    url=\"https://github.com/cvzone/cvzone/blob/master/Results/cvzoneLogo.png?raw=true\",\n",
    "    keepTransparency=False,\n",
    ")\n",
    "imgPNG = cv2.resize(imgPNG, (0, 0), None, 3, 3)\n",
    "\n",
    "cv2.imshow(\"Image Normal\", imgNormal)\n",
    "cv2.imshow(\"Transparent Image\", imgPNG)\n",
    "\n",
    "if cv2.waitKey(0) == ord(\"q\"):\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Overlay PNG**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function overlayPNG in module cvzone.Utils:\n",
      "\n",
      "overlayPNG(imgBack, imgFront, pos=[0, 0])\n",
      "    Overlay a PNG image with transparency onto another image using alpha blending.\n",
      "    The function handles out-of-bound positions, including negative coordinates, by cropping\n",
      "    the overlay image accordingly. Edges are smoothed using alpha blending.\n",
      "\n",
      "    :param imgBack: The background image, a NumPy array of shape (height, width, 3) or (height, width, 4).\n",
      "    :param imgFront: The foreground PNG image to overlay, a NumPy array of shape (height, width, 4).\n",
      "    :param pos: A list specifying the x and y coordinates (in pixels) at which to overlay the image.\n",
      "                Can be negative or cause the overlay image to go out-of-bounds.\n",
      "    :return: A new image with the overlay applied, a NumPy array of shape like `imgBack`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(cvzone.overlayPNG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import cvzone\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "imgPNG = cvzone.downloadImageFromUrl(\n",
    "    url=\"https://github.com/cvzone/cvzone/blob/master/Results/cvzoneLogo.png?raw=true\",\n",
    "    keepTransparency=True,\n",
    ")\n",
    "\n",
    "# imgPNG = cv2.imread(\"cvzoneLogo.png\",cv2.IMREAD_UNCHANGED)\n",
    "\n",
    "\n",
    "while True:\n",
    "    success, img = cap.read()\n",
    "\n",
    "    imgOverlay = cvzone.overlayPNG(imgBack=img, imgFront=imgPNG, pos=[-30, 50])\n",
    "    imgOverlay = cvzone.overlayPNG(imgBack=img, imgFront=imgPNG, pos=[200, 200])\n",
    "    imgOverlay = cvzone.overlayPNG(imgBack=img, imgFront=imgPNG, pos=[500, 400])\n",
    "\n",
    "    cv2.imshow(\"imgOverlay\", imgOverlay)\n",
    "\n",
    "    if cv2.waitKey(1) == ord(\"q\"):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Rotate Image**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function rotateImage in module cvzone.Utils:\n",
      "\n",
      "rotateImage(imgInput, angle, scale=1, keepSize=False)\n",
      "    Rotates an image around it's center while optionally keeping the original image dimensions.\n",
      "\n",
      "    :param imgInput: The input image to be rotated. Should be an ndarray.\n",
      "    :param angle: The angle by which the image is to be rotated. Should be a float.\n",
      "    :param scale: A scaling factor that allows the image to be scaled while rotating. Default is 1. Optional.\n",
      "    :param keepSize: If True, keeps the dimensions of the rotated image the same as the input.\n",
      "                     If False, adjusts dimensions to fit the entire rotated image. Default is False. Optional.\n",
      "\n",
      "    :return: The rotated image as an ndarray.\n",
      "\n",
      "    Example:\n",
      "        rotated_img = rotateImage(img, 90, keepSize=True)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(cvzone.rotateImage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import cvzone\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    success, img = cap.read()\n",
    "\n",
    "    rotated_img = cvzone.rotateImage(img, angle=60, scale=1, keepSize=False)\n",
    "    # rotated_img = cvzone.rotateImage(img, 60, scale=1, keepSize=True)\n",
    "\n",
    "    cv2.imshow(\"Image Rotated 60\", rotated_img)\n",
    "    if cv2.waitKey(1) == ord(\"q\"):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Stack Images**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function stackImages in module cvzone.Utils:\n",
      "\n",
      "stackImages(_imgList, cols, scale)\n",
      "    Stack Images together to display in a single window\n",
      "    :param _imgList: list of images to stack\n",
      "    :param cols: the num of img in a row\n",
      "    :param scale: bigger~1+ ans smaller~1-\n",
      "    :return: Stacked Image\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(cvzone.stackImages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import cvzone\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    # Read image frame from camera\n",
    "    success, img = cap.read()\n",
    "\n",
    "    # Convert the image to grayscale\n",
    "    imgGray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Resize the image to be smaller (0.1x of original size)\n",
    "    imgSmall = cv2.resize(img, (0, 0), None, 0.1, 0.1)\n",
    "\n",
    "    # Resize the image to be larger (3x of original size)\n",
    "    imgBig = cv2.resize(img, (0, 0), None, 3, 3)\n",
    "\n",
    "    # Apply Canny edge detection on the grayscale image\n",
    "    imgCanny = cv2.Canny(imgGray, 50, 150)\n",
    "\n",
    "    # Convert the image to HSV color space\n",
    "    imgHSV = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "    # Create a list of all processed images\n",
    "    imgList = [img, imgGray, imgCanny, imgSmall, imgBig, imgHSV]\n",
    "\n",
    "    # Stack the images together using cvzone's stackImages function\n",
    "    stackedImg = cvzone.stackImages(imgList, cols=3, scale=0.5)\n",
    "\n",
    "    cv2.imshow(\"Stacked Image\", stackedImg)\n",
    "\n",
    "    if cv2.waitKey(1) == ord(\"q\"):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **FPS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class FPS in module cvzone.FPS:\n",
      "\n",
      "class FPS(builtins.object)\n",
      " |  FPS(avgCount=30)\n",
      " |\n",
      " |  FPS class for calculating and displaying the Frames Per Second in a video stream.\n",
      " |\n",
      " |  Attributes:\n",
      " |      pTime (float): Previous time stamp.\n",
      " |      frameTimes (list): List to keep track of frame times.\n",
      " |      avgCount (int): Number of frames over which to average the FPS.\n",
      " |\n",
      " |  Methods defined here:\n",
      " |\n",
      " |  __init__(self, avgCount=30)\n",
      " |      Initialize FPS class.\n",
      " |\n",
      " |      :param avgCount: Number of frames over which to average the FPS, default is 30.\n",
      " |\n",
      " |  update(self, img=None, pos=(20, 50), bgColor=(255, 0, 255), textColor=(255, 255, 255), scale=3, thickness=3)\n",
      " |      Update the frame rate and optionally display it on the image.\n",
      " |\n",
      " |      :param img: Image to display FPS on. If None, just returns the FPS value.\n",
      " |      :param pos: Position to display FPS on the image.\n",
      " |      :param bgColor: Background color of the FPS text.\n",
      " |      :param textColor: Text color of the FPS display.\n",
      " |      :param scale: Font scale of the FPS text.\n",
      " |      :param thickness: Thickness of the FPS text.\n",
      " |      :return: FPS value, and optionally the image with FPS drawn on it.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |\n",
      " |  __dict__\n",
      " |      dictionary for instance variables\n",
      " |\n",
      " |  __weakref__\n",
      " |      list of weak references to the object\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from cvzone.FPS import FPS\n",
    "\n",
    "help(FPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cvzone.FPS import FPS\n",
    "import cv2\n",
    "\n",
    "# Initialize the FPS class with an average count of 30 frames for smoothing\n",
    "fpsReader = FPS(avgCount=30)\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "cap.set(cv2.CAP_PROP_FPS, 30)\n",
    "\n",
    "while True:\n",
    "    success, img = cap.read()\n",
    "\n",
    "    # Update the FPS counter and draw the FPS on the image\n",
    "    # fpsReader.update returns the current FPS and the updated image\n",
    "    fps, img = fpsReader.update(\n",
    "        img,\n",
    "        pos=(4, 40),\n",
    "        bgColor=(255, 0, 255),\n",
    "        textColor=(255, 255, 255),\n",
    "        scale=3,\n",
    "        thickness=3,\n",
    "    )\n",
    "\n",
    "    cv2.imshow(\"Image\", img)\n",
    "\n",
    "    if cv2.waitKey(1) == ord(\"q\"):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Finding Contours**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function findContours in module cvzone.Utils:\n",
      "\n",
      "findContours(img, imgPre, minArea=1000, maxArea=inf, sort=True, filter=None, drawCon=True, c=(255, 0, 0), ct=(255, 0, 255), retrType=0, approxType=1)\n",
      "    Finds Contours in an image.\n",
      "    Sorts them based on area\n",
      "    Can use filtration to get based on x corner points\n",
      "    e.g. filter = [3,4] will return triangles and rectangles both\n",
      "\n",
      "    :param img: Image on which we want to draw.\n",
      "    :param imgPre: Image on which we want to find contours.\n",
      "    :param minArea: Minimum Area to detect as valid contour.\n",
      "    :param maxArea: Maximum Area to detect as valid contour.\n",
      "    :param sort: True will sort the contours by area (biggest first).\n",
      "    :param filter: List of filters based on the corner points e.g. [3, 4, 5].\n",
      "                   If None, no filtering will be done.\n",
      "    :param drawCon: Draw contours boolean.\n",
      "    :param c: Color to draw the contours.\n",
      "    :param ct: Color for Text\n",
      "    :param retrType: Retrieval type for cv2.findContours (default is cv2.RETR_EXTERNAL).\n",
      "    :param approxType: Approximation type for cv2.findContours (default is cv2.CHAIN_APPROX_NONE).\n",
      "\n",
      "    :return: Found contours with [contours, Area, BoundingBox, Center].\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(cvzone.findContours)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import cvzone\n",
    "import numpy as np\n",
    "\n",
    "imgShapes = cvzone.downloadImageFromUrl(\n",
    "    url=\"https://github.com/cvzone/cvzone/blob/master/Results/shapes.png?raw=true\"\n",
    ")\n",
    "\n",
    "# Perform edge detection using the Canny algorithm\n",
    "imgCanny = cv2.Canny(imgShapes, 50, 150)\n",
    "\n",
    "# Dilate the edges to strengthen the detected contours\n",
    "imgDilated = cv2.dilate(imgCanny, np.ones((5, 5), np.uint8), iterations=1)\n",
    "\n",
    "# Find contours in the image without any corner filtering\n",
    "imgContours, conFound = cvzone.findContours(\n",
    "    imgShapes,\n",
    "    imgDilated,\n",
    "    minArea=1000,\n",
    "    sort=True,\n",
    "    filter=None,\n",
    "    drawCon=True,\n",
    "    c=(0, 0, 255),\n",
    "    ct=(0, 0, 0),\n",
    "    retrType=cv2.RETR_EXTERNAL,\n",
    "    approxType=cv2.CHAIN_APPROX_NONE,\n",
    ")\n",
    "\n",
    "# Find contours in the image and filter them based on corner points (either 3 or 4 corners)\n",
    "imgContoursFiltered, conFoundFiltered = cvzone.findContours(\n",
    "    imgShapes,\n",
    "    imgDilated,\n",
    "    minArea=1000,\n",
    "    sort=True,\n",
    "    filter=[3, 4],\n",
    "    drawCon=True,\n",
    "    c=(0, 0, 255),\n",
    "    ct=(0, 0, 0),\n",
    "    retrType=cv2.RETR_EXTERNAL,\n",
    "    approxType=cv2.CHAIN_APPROX_NONE,\n",
    ")\n",
    "\n",
    "cv2.imshow(\"Image\", imgShapes)\n",
    "\n",
    "# Display the image with all found contours\n",
    "cv2.imshow(\"Image Contours\", imgContours)\n",
    "\n",
    "# Display the image with filtered contours (either 3 or 4 corners)\n",
    "cv2.imshow(\"Image Contours Filtered\", imgContoursFiltered)\n",
    "\n",
    "if cv2.waitKey(0) == ord(\"q\"):\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Face Detection Module**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class FaceDetector in module cvzone.FaceDetectionModule:\n",
      "\n",
      "class FaceDetector(builtins.object)\n",
      " |  FaceDetector(minDetectionCon=0.5, modelSelection=0)\n",
      " |\n",
      " |  Find faces in realtime using the light weight model provided in the mediapipe\n",
      " |  library.\n",
      " |\n",
      " |  Methods defined here:\n",
      " |\n",
      " |  __init__(self, minDetectionCon=0.5, modelSelection=0)\n",
      " |      :param minDetectionCon: Minimum confidence value ([0.0, 1.0]) for face\n",
      " |      detection to be considered successful. See details in\n",
      " |      https://solutions.mediapipe.dev/face_detection#min_detection_confidence.\n",
      " |\n",
      " |      :param modelSelection: 0 or 1. 0 to select a short-range model that works\n",
      " |      best for faces within 2 meters from the camera, and 1 for a full-range\n",
      " |      model best for faces within 5 meters. See details in\n",
      " |      https://solutions.mediapipe.dev/face_detection#model_selection.\n",
      " |\n",
      " |  findFaces(self, img, draw=True)\n",
      " |      Find faces in an image and return the bbox info\n",
      " |      :param img: Image to find the faces in.\n",
      " |      :param draw: Flag to draw the output on the image.\n",
      " |      :return: Image with or without drawings.\n",
      " |               Bounding Box list.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |\n",
      " |  __dict__\n",
      " |      dictionary for instance variables\n",
      " |\n",
      " |  __weakref__\n",
      " |      list of weak references to the object\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from cvzone.FaceDetectionModule import FaceDetector\n",
    "\n",
    "help(FaceDetector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cvzone\n",
    "from cvzone.FaceDetectionModule import FaceDetector\n",
    "import cv2\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Initialize the FaceDetector object\n",
    "detector = FaceDetector(minDetectionCon=0.5, modelSelection=0)\n",
    "\n",
    "while True:\n",
    "    success, img = cap.read()\n",
    "\n",
    "    # Detect faces in the image\n",
    "    img, bboxs = detector.findFaces(img, draw=False)\n",
    "\n",
    "    # Check if any face is detected\n",
    "    if bboxs:\n",
    "        # Loop through each bounding box\n",
    "        for bbox in bboxs:\n",
    "            # bbox contains 'id', 'bbox', 'score', 'center'\n",
    "\n",
    "            center = bbox[\"center\"]\n",
    "            x, y, w, h = bbox[\"bbox\"]\n",
    "            score = int(bbox[\"score\"][0] * 100)\n",
    "\n",
    "            cv2.circle(img, center, 5, (255, 0, 255), cv2.FILLED)\n",
    "            cvzone.putTextRect(img, f\"{score}%\", (x, y - 10))\n",
    "            cvzone.cornerRect(img, (x, y, w, h))\n",
    "\n",
    "    cv2.imshow(\"Image\", img)\n",
    "\n",
    "    if cv2.waitKey(5) & 0xFF == ord(\"q\"):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['id', 'bbox', 'score', 'center'])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bboxs[0].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Face Mesh Module**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class FaceMeshDetector in module cvzone.FaceMeshModule:\n",
      "\n",
      "class FaceMeshDetector(builtins.object)\n",
      " |  FaceMeshDetector(staticMode=False, maxFaces=2, minDetectionCon=0.5, minTrackCon=0.5)\n",
      " |\n",
      " |  Face Mesh Detector to find 468 Landmarks using the mediapipe library.\n",
      " |  Helps acquire the landmark points in pixel format\n",
      " |\n",
      " |  Methods defined here:\n",
      " |\n",
      " |  __init__(self, staticMode=False, maxFaces=2, minDetectionCon=0.5, minTrackCon=0.5)\n",
      " |      :param staticMode: In static mode, detection is done on each image: slower\n",
      " |      :param maxFaces: Maximum number of faces to detect\n",
      " |      :param minDetectionCon: Minimum Detection Confidence Threshold\n",
      " |      :param minTrackCon: Minimum Tracking Confidence Threshold\n",
      " |\n",
      " |  findDistance(self, p1, p2, img=None)\n",
      " |      Find the distance between two landmarks based on their\n",
      " |      index numbers.\n",
      " |      :param p1: Point1\n",
      " |      :param p2: Point2\n",
      " |      :param img: Image to draw on.\n",
      " |      :param draw: Flag to draw the output on the image.\n",
      " |      :return: Distance between the points\n",
      " |               Image with output drawn\n",
      " |               Line information\n",
      " |\n",
      " |  findFaceMesh(self, img, draw=True)\n",
      " |      Finds face landmarks in BGR Image.\n",
      " |      :param img: Image to find the face landmarks in.\n",
      " |      :param draw: Flag to draw the output on the image.\n",
      " |      :return: Image with or without drawings\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |\n",
      " |  __dict__\n",
      " |      dictionary for instance variables\n",
      " |\n",
      " |  __weakref__\n",
      " |      list of weak references to the object\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from cvzone.FaceMeshModule import FaceMeshDetector\n",
    "\n",
    "help(FaceMeshDetector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cvzone.FaceMeshModule import FaceMeshDetector\n",
    "import cv2\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Initialize FaceMeshDetector object\n",
    "detector = FaceMeshDetector(\n",
    "    staticMode=False,\n",
    "    maxFaces=2,\n",
    "    minDetectionCon=0.5,\n",
    "    minTrackCon=0.5,\n",
    ")\n",
    "\n",
    "while True:\n",
    "    success, img = cap.read()\n",
    "\n",
    "    # Find face mesh in the image\n",
    "    img, faces = detector.findFaceMesh(img, draw=True)\n",
    "\n",
    "    # Check if any faces are detected\n",
    "    if faces:\n",
    "        # Loop through each detected face\n",
    "        for face in faces:\n",
    "            # Get specific points for the eye\n",
    "            leftEyeUpPoint = face[159]\n",
    "            leftEyeDownPoint = face[23]\n",
    "            cv2.circle(img, leftEyeUpPoint, 3, (0, 255, 0), -1)\n",
    "            cv2.circle(img, leftEyeDownPoint, 3, (0, 255, 0), -1)\n",
    "\n",
    "            # Calculate the vertical distance between the eye points\n",
    "            # info: Additional information (like coordinates)\n",
    "            leftEyeVerticalDistance, info = detector.findDistance(\n",
    "                leftEyeUpPoint, leftEyeDownPoint\n",
    "            )\n",
    "\n",
    "            # Print the vertical distance for debugging or information\n",
    "            # print(leftEyeVerticalDistance)\n",
    "\n",
    "    cv2.imshow(\"Image\", img)\n",
    "\n",
    "    if cv2.waitKey(5) & 0xFF == ord(\"q\"):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Hand Tracking Module**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class HandDetector in module cvzone.HandTrackingModule:\n",
      "\n",
      "class HandDetector(builtins.object)\n",
      " |  HandDetector(staticMode=False, maxHands=2, modelComplexity=1, detectionCon=0.5, minTrackCon=0.5)\n",
      " |\n",
      " |  Finds Hands using the mediapipe library. Exports the landmarks\n",
      " |  in pixel format. Adds extra functionalities like finding how\n",
      " |  many fingers are up or the distance between two fingers. Also\n",
      " |  provides bounding box info of the hand found.\n",
      " |\n",
      " |  Methods defined here:\n",
      " |\n",
      " |  __init__(self, staticMode=False, maxHands=2, modelComplexity=1, detectionCon=0.5, minTrackCon=0.5)\n",
      " |      :param mode: In static mode, detection is done on each image: slower\n",
      " |      :param maxHands: Maximum number of hands to detect\n",
      " |      :param modelComplexity: Complexity of the hand landmark model: 0 or 1.\n",
      " |      :param detectionCon: Minimum Detection Confidence Threshold\n",
      " |      :param minTrackCon: Minimum Tracking Confidence Threshold\n",
      " |\n",
      " |  findDistance(self, p1, p2, img=None, color=(255, 0, 255), scale=5)\n",
      " |      Find the distance between two landmarks input should be (x1,y1) (x2,y2)\n",
      " |      :param p1: Point1 (x1,y1)\n",
      " |      :param p2: Point2 (x2,y2)\n",
      " |      :param img: Image to draw output on. If no image input output img is None\n",
      " |      :return: Distance between the points\n",
      " |               Image with output drawn\n",
      " |               Line information\n",
      " |\n",
      " |  findHands(self, img, draw=True, flipType=True)\n",
      " |      Finds hands in a BGR image.\n",
      " |      :param img: Image to find the hands in.\n",
      " |      :param draw: Flag to draw the output on the image.\n",
      " |      :return: Image with or without drawings\n",
      " |\n",
      " |  fingersUp(self, myHand)\n",
      " |      Finds how many fingers are open and returns in a list.\n",
      " |      Considers left and right hands separately\n",
      " |      :return: List of which fingers are up\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |\n",
      " |  __dict__\n",
      " |      dictionary for instance variables\n",
      " |\n",
      " |  __weakref__\n",
      " |      list of weak references to the object\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from cvzone.HandTrackingModule import HandDetector\n",
    "\n",
    "help(HandDetector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cvzone.HandTrackingModule import HandDetector\n",
    "import cv2\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Initialize the HandDetector class with the given parameters\n",
    "detector = HandDetector(\n",
    "    staticMode=False,\n",
    "    maxHands=2,\n",
    "    modelComplexity=1,\n",
    "    detectionCon=0.5,\n",
    "    minTrackCon=0.5,\n",
    ")\n",
    "\n",
    "while True:\n",
    "    success, img = cap.read()\n",
    "\n",
    "    # img = cv2.flip(img, 1)\n",
    "\n",
    "    # Find hands in the current frame\n",
    "    hands, img = detector.findHands(img, draw=True, flipType=True, draw_boxes=False)\n",
    "\n",
    "    # Check if any hands are detected\n",
    "    if hands:\n",
    "        # Information for the first hand detected\n",
    "        hand1 = hands[0]\n",
    "        lmList1 = hand1[\"lmList\"]\n",
    "        bbox1 = hand1[\"bbox\"]\n",
    "        center1 = hand1[\"center\"]\n",
    "        handType1 = hand1[\"type\"]\n",
    "\n",
    "        # Count the number of fingers up for the first hand\n",
    "        fingers1 = detector.fingersUp(hand1)\n",
    "        print(f\"{handType1} = {fingers1.count(1)}\", end=\" \")\n",
    "\n",
    "        # Calculate distance between specific landmarks on the first hand and draw it on the image\n",
    "        length, info, img = detector.findDistance(\n",
    "            lmList1[8][0:2], lmList1[12][0:2], img, color=(255, 0, 255), scale=5\n",
    "        )\n",
    "\n",
    "        # Check if a second hand is detected\n",
    "        if len(hands) == 2:\n",
    "            # Information for the second hand\n",
    "            hand2 = hands[1]\n",
    "            lmList2 = hand2[\"lmList\"]\n",
    "            bbox2 = hand2[\"bbox\"]\n",
    "            center2 = hand2[\"center\"]\n",
    "            handType2 = hand2[\"type\"]\n",
    "\n",
    "            # Count the number of fingers up for the second hand\n",
    "            fingers2 = detector.fingersUp(hand2)\n",
    "            print(f\"{handType2} = {fingers2.count(1)}\", end=\" \")\n",
    "\n",
    "            # Calculate distance between the index fingers of both hands and draw it on the image\n",
    "            length, info, img = detector.findDistance(\n",
    "                lmList1[8][0:2], lmList2[8][0:2], img, color=(255, 0, 0), scale=5\n",
    "            )\n",
    "\n",
    "        print(\" \")  # New line for better readability of the printed output\n",
    "\n",
    "    cv2.imshow(\"Image\", img)\n",
    "\n",
    "    if cv2.waitKey(5) & 0xFF == ord(\"q\"):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Pose Estimation Module**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class PoseDetector in module cvzone.PoseModule:\n",
      "\n",
      "class PoseDetector(builtins.object)\n",
      " |  PoseDetector(staticMode=False, modelComplexity=1, smoothLandmarks=True, enableSegmentation=False, smoothSegmentation=True, detectionCon=0.5, trackCon=0.5)\n",
      " |\n",
      " |  Estimates Pose points of a human body using the mediapipe library.\n",
      " |\n",
      " |  Methods defined here:\n",
      " |\n",
      " |  __init__(self, staticMode=False, modelComplexity=1, smoothLandmarks=True, enableSegmentation=False, smoothSegmentation=True, detectionCon=0.5, trackCon=0.5)\n",
      " |      :param mode: In static mode, detection is done on each image: slower\n",
      " |      :param upBody: Upper boy only flag\n",
      " |      :param smooth: Smoothness Flag\n",
      " |      :param detectionCon: Minimum Detection Confidence Threshold\n",
      " |      :param trackCon: Minimum Tracking Confidence Threshold\n",
      " |\n",
      " |  angleCheck(self, myAngle, targetAngle, offset=20)\n",
      " |\n",
      " |  findAngle(self, p1, p2, p3, img=None, color=(255, 0, 255), scale=5)\n",
      " |      Finds angle between three points.\n",
      " |\n",
      " |      :param p1: Point1 - (x1,y1)\n",
      " |      :param p2: Point2 - (x2,y2)\n",
      " |      :param p3: Point3 - (x3,y3)\n",
      " |      :param img: Image to draw output on. If no image input output img is None\n",
      " |      :return:\n",
      " |\n",
      " |  findDistance(self, p1, p2, img=None, color=(255, 0, 255), scale=5)\n",
      " |      Find the distance between two landmarks input should be (x1,y1) (x2,y2)\n",
      " |      :param p1: Point1 (x1,y1)\n",
      " |      :param p2: Point2 (x2,y2)\n",
      " |      :param img: Image to draw output on. If no image input output img is None\n",
      " |      :return: Distance between the points\n",
      " |               Image with output drawn\n",
      " |               Line information\n",
      " |\n",
      " |  findPose(self, img, draw=True)\n",
      " |      Find the pose landmarks in an Image of BGR color space.\n",
      " |      :param img: Image to find the pose in.\n",
      " |      :param draw: Flag to draw the output on the image.\n",
      " |      :return: Image with or without drawings\n",
      " |\n",
      " |  findPosition(self, img, draw=True, bboxWithHands=False)\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |\n",
      " |  __dict__\n",
      " |      dictionary for instance variables\n",
      " |\n",
      " |  __weakref__\n",
      " |      list of weak references to the object\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from cvzone.PoseModule import PoseDetector\n",
    "\n",
    "help(PoseDetector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cvzone.PoseModule import PoseDetector\n",
    "import cv2\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Initialize the PoseDetector class with the given parameters\n",
    "detector = PoseDetector(\n",
    "    staticMode=False,\n",
    "    modelComplexity=1,\n",
    "    smoothLandmarks=True,\n",
    "    enableSegmentation=False,\n",
    "    smoothSegmentation=True,\n",
    "    detectionCon=0.5,\n",
    "    trackCon=0.5,\n",
    ")\n",
    "\n",
    "while True:\n",
    "    success, img = cap.read()\n",
    "\n",
    "    img = detector.findPose(img)\n",
    "\n",
    "    # Find the landmarks, bounding box, and center of the body in the frame\n",
    "    lmList, bboxInfo = detector.findPosition(img, draw=False, bboxWithHands=False)\n",
    "\n",
    "    if lmList:\n",
    "        # Calculate the distance between landmarks 11 and 15 and draw it on the image\n",
    "        length, img, info = detector.findDistance(\n",
    "            lmList[11][0:2], lmList[15][0:2], img=img, color=(255, 0, 0), scale=10\n",
    "        )\n",
    "\n",
    "        # Calculate the angle between landmarks 11, 13, and 15 and draw it on the image\n",
    "        angle, img = detector.findAngle(\n",
    "            lmList[11][0:2],\n",
    "            lmList[13][0:2],\n",
    "            lmList[15][0:2],\n",
    "            img=img,\n",
    "            color=(0, 0, 255),\n",
    "            scale=10,\n",
    "        )\n",
    "\n",
    "    # Display the frame in a window\n",
    "    cv2.imshow(\"Image\", img)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
